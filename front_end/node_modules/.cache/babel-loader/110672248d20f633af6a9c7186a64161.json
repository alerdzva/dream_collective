{"ast":null,"code":"var __awaiter = this && this.__awaiter || function (thisArg, _arguments, P, generator) {\n  function adopt(value) {\n    return value instanceof P ? value : new P(function (resolve) {\n      resolve(value);\n    });\n  }\n\n  return new (P || (P = Promise))(function (resolve, reject) {\n    function fulfilled(value) {\n      try {\n        step(generator.next(value));\n      } catch (e) {\n        reject(e);\n      }\n    }\n\n    function rejected(value) {\n      try {\n        step(generator[\"throw\"](value));\n      } catch (e) {\n        reject(e);\n      }\n    }\n\n    function step(result) {\n      result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected);\n    }\n\n    step((generator = generator.apply(thisArg, _arguments || [])).next());\n  });\n};\n\nvar __generator = this && this.__generator || function (thisArg, body) {\n  var _ = {\n    label: 0,\n    sent: function () {\n      if (t[0] & 1) throw t[1];\n      return t[1];\n    },\n    trys: [],\n    ops: []\n  },\n      f,\n      y,\n      t,\n      g;\n  return g = {\n    next: verb(0),\n    \"throw\": verb(1),\n    \"return\": verb(2)\n  }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function () {\n    return this;\n  }), g;\n\n  function verb(n) {\n    return function (v) {\n      return step([n, v]);\n    };\n  }\n\n  function step(op) {\n    if (f) throw new TypeError(\"Generator is already executing.\");\n\n    while (_) try {\n      if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n      if (y = 0, t) op = [op[0] & 2, t.value];\n\n      switch (op[0]) {\n        case 0:\n        case 1:\n          t = op;\n          break;\n\n        case 4:\n          _.label++;\n          return {\n            value: op[1],\n            done: false\n          };\n\n        case 5:\n          _.label++;\n          y = op[1];\n          op = [0];\n          continue;\n\n        case 7:\n          op = _.ops.pop();\n\n          _.trys.pop();\n\n          continue;\n\n        default:\n          if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) {\n            _ = 0;\n            continue;\n          }\n\n          if (op[0] === 3 && (!t || op[1] > t[0] && op[1] < t[3])) {\n            _.label = op[1];\n            break;\n          }\n\n          if (op[0] === 6 && _.label < t[1]) {\n            _.label = t[1];\n            t = op;\n            break;\n          }\n\n          if (t && _.label < t[2]) {\n            _.label = t[2];\n\n            _.ops.push(op);\n\n            break;\n          }\n\n          if (t[2]) _.ops.pop();\n\n          _.trys.pop();\n\n          continue;\n      }\n\n      op = body.call(thisArg, _);\n    } catch (e) {\n      op = [6, e];\n      y = 0;\n    } finally {\n      f = t = 0;\n    }\n\n    if (op[0] & 5) throw op[1];\n    return {\n      value: op[0] ? op[1] : void 0,\n      done: true\n    };\n  }\n};\n\nvar __spreadArrays = this && this.__spreadArrays || function () {\n  for (var s = 0, i = 0, il = arguments.length; i < il; i++) s += arguments[i].length;\n\n  for (var r = Array(s), k = 0, i = 0; i < il; i++) for (var a = arguments[i], j = 0, jl = a.length; j < jl; j++, k++) r[k] = a[j];\n\n  return r;\n};\n\nimport { r as registerInstance, h, c as createEvent, H as Host, g as getElement } from './index-39969785.js';\nimport { Logger, browserOrNode, I18n } from '@aws-amplify/core';\nimport '@aws-amplify/auth';\nimport { T as Translations } from './Translations-392acb9b.js';\nimport { d as NO_INTERACTIONS_MODULE_FOUND } from './constants-d1abe7de.js';\nimport { Interactions } from '@aws-amplify/interactions'; // AudioRecorder settings\n\nvar RECORDER_EXPORT_MIME_TYPE = 'application/octet-stream';\nvar DEFAULT_EXPORT_SAMPLE_RATE = 16000;\nvar FFT_SIZE = 2048; // window size in samples for Fast Fourier Transform (FFT)\n\nvar FFT_MAX_DECIBELS = -10; // maximum power value in the scaling range for the FFT analysis data\n\nvar FFT_MIN_DECIBELS = -90; // minimum power value in the scaling range for the FFT analysis data\n\nvar FFT_SMOOTHING_TIME_CONSTANT = 0.85; // averaging constant with the last analysis frame\n\n/**\n * Merges multiple buffers into one.\n */\n\nvar mergeBuffers = function (bufferArray, recLength) {\n  var result = new Float32Array(recLength);\n  var offset = 0;\n\n  for (var i = 0; i < bufferArray.length; i++) {\n    result.set(bufferArray[i], offset);\n    offset += bufferArray[i].length;\n  }\n\n  return result;\n};\n/**\n * Downsamples audio to desired export sample rate.\n */\n\n\nvar downsampleBuffer = function (buffer, recordSampleRate, exportSampleRate) {\n  if (exportSampleRate === recordSampleRate) {\n    return buffer;\n  }\n\n  var sampleRateRatio = recordSampleRate / exportSampleRate;\n  var newLength = Math.round(buffer.length / sampleRateRatio);\n  var result = new Float32Array(newLength);\n  var offsetResult = 0;\n  var offsetBuffer = 0;\n\n  while (offsetResult < result.length) {\n    var nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);\n    var accum = 0,\n        count = 0;\n\n    for (var i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {\n      accum += buffer[i];\n      count++;\n    }\n\n    result[offsetResult] = accum / count;\n    offsetResult++;\n    offsetBuffer = nextOffsetBuffer;\n  }\n\n  return result;\n};\n/**\n * converts raw audio values to 16 bit pcm.\n */\n\n\nvar floatTo16BitPCM = function (output, offset, input) {\n  var byteOffset = offset;\n\n  for (var i = 0; i < input.length; i++, byteOffset += 2) {\n    var s = Math.max(-1, Math.min(1, input[i]));\n    output.setInt16(byteOffset, s < 0 ? s * 0x8000 : s * 0x7fff, true);\n  }\n};\n/**\n * Write given strings in big-endian order.\n */\n\n\nvar writeString = function (view, offset, string) {\n  for (var i = 0; i < string.length; i++) {\n    view.setUint8(offset + i, string.charCodeAt(i));\n  }\n};\n/**\n * Encodes raw pcm audio into a wav file.\n */\n\n\nvar encodeWAV = function (samples, exportSampleRate) {\n  /**\n   * WAV file consists of three parts: RIFF header, WAVE subchunk, and data subchunk. We precompute the size of them.\n   */\n  var audioSize = samples.length * 2; // We use 16-bit samples, so we have (2 * sampleLength) bytes.\n\n  var fmtSize = 24; // Byte size of the fmt subchunk: 24 bytes that the audio information that we'll set below.\n\n  var dataSize = 8 + audioSize; // Byte size of the data subchunk: raw sound data plus 8 bytes for the subchunk descriptions.\n\n  var totalByteSize = 12 + fmtSize + dataSize; // Byte size of the whole file, including the chunk header / descriptor.\n  // create DataView object to write byte values into\n\n  var buffer = new ArrayBuffer(totalByteSize); // buffer to write the chunk values in.\n\n  var view = new DataView(buffer);\n  /**\n   * Start writing the .wav file. We write top to bottom, so byte offset (first numeric argument) increases strictly.\n   */\n  // RIFF header\n\n  writeString(view, 0, 'RIFF'); // At offset 0, write the letters \"RIFF\"\n\n  view.setUint32(4, fmtSize + dataSize, true); // At offset 4, write the size of fmt and data chunk size combined.\n\n  writeString(view, 8, 'WAVE'); // At offset 8, write the format type \"WAVE\"\n  // fmt subchunk\n\n  writeString(view, 12, 'fmt '); //chunkdId 'fmt '\n\n  view.setUint32(16, fmtSize - 8, true); // fmt subchunk size below this value. We set 8 bytes already, so subtract 8 bytes from fmtSize.\n\n  view.setUint16(20, 1, true); // Audiio format code, which is 1 for PCM.\n\n  view.setUint16(22, 1, true); // Number of audio channels. We use mono, ie 1.\n\n  view.setUint32(24, exportSampleRate, true); // Sample rate of the audio file.\n\n  view.setUint32(28, exportSampleRate * 2, true); // Data rate, or # of data bytes per second. Since each sample is 2 bytes, this is 2 * sampleRate.\n\n  view.setUint16(32, 2, true); // block align, # of bytes per sample including all channels, ie. 2 bytes.\n\n  view.setUint16(34, 16, true); // bits per sample, ie. 16 bits\n  // data subchunk\n\n  writeString(view, 36, 'data'); // write the chunkId 'data'\n\n  view.setUint32(40, audioSize, true); // Audio byte size\n\n  floatTo16BitPCM(view, 44, samples); // raw pcm values then go here.\n\n  return view;\n};\n/**\n * Given arrays of raw pcm audio, downsamples the audio to desired sample rate and encodes it to a wav audio file.\n *\n * @param recBuffer {Float32Array[]} - 2d float array containing the recorded raw audio\n * @param recLength {number} - total length of recorded audio\n * @param recordSampleRate {number} - sample rate of the recorded audio\n * @param exportSampleRate {number} - desired sample rate of the exported file\n */\n\n\nvar exportBuffer = function (recBuffer, recLength, recordSampleRate, exportSampleRate) {\n  var mergedBuffers = mergeBuffers(recBuffer, recLength);\n  var downsampledBuffer = downsampleBuffer(mergedBuffers, recordSampleRate, exportSampleRate);\n  var encodedWav = encodeWAV(downsampledBuffer, exportSampleRate);\n  var audioBlob = new Blob([encodedWav], {\n    type: RECORDER_EXPORT_MIME_TYPE\n  });\n  return audioBlob;\n};\n\nvar logger = new Logger('AudioRecorder');\n\nvar AudioRecorder =\n/** @class */\nfunction () {\n  function AudioRecorder(options) {\n    // input mic stream is stored in a buffer\n    this.streamBuffer = [];\n    this.streamBufferLength = 0;\n    this.recording = false;\n    this.options = options;\n  }\n  /**\n   * This must be called first to enable audio context and request microphone access.\n   * Once access granted, it connects all the necessary audio nodes to the context so that it can begin recording or playing.\n   */\n\n\n  AudioRecorder.prototype.init = function () {\n    return __awaiter(this, void 0, void 0, function () {\n      var _this = this;\n\n      return __generator(this, function (_a) {\n        switch (_a.label) {\n          case 0:\n            if (!browserOrNode().isBrowser) return [3\n            /*break*/\n            , 2];\n            window.AudioContext = window.AudioContext || window.webkitAudioContext;\n            this.audioContext = new AudioContext();\n            return [4\n            /*yield*/\n            , navigator.mediaDevices.getUserMedia({\n              audio: true\n            }).then(function (stream) {\n              _this.audioSupported = true;\n\n              _this.setupAudioNodes(stream);\n            }).catch(function () {\n              _this.audioSupported = false;\n              return Promise.reject('Audio is not supported');\n            })];\n\n          case 1:\n            _a.sent();\n\n            return [3\n            /*break*/\n            , 3];\n\n          case 2:\n            this.audioSupported = false;\n            return [2\n            /*return*/\n            , Promise.reject('Audio is not supported')];\n\n          case 3:\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n  /**\n   * Setup audio nodes after successful `init`.\n   */\n\n\n  AudioRecorder.prototype.setupAudioNodes = function (stream) {\n    return __awaiter(this, void 0, void 0, function () {\n      var err_1, sourceNode, processorNode, analyserNode;\n\n      var _this = this;\n\n      return __generator(this, function (_a) {\n        switch (_a.label) {\n          case 0:\n            _a.trys.push([0, 2,, 3]);\n\n            return [4\n            /*yield*/\n            , this.audioContext.resume()];\n\n          case 1:\n            _a.sent();\n\n            return [3\n            /*break*/\n            , 3];\n\n          case 2:\n            err_1 = _a.sent();\n            logger.error(err_1);\n            return [3\n            /*break*/\n            , 3];\n\n          case 3:\n            sourceNode = this.audioContext.createMediaStreamSource(stream);\n            processorNode = this.audioContext.createScriptProcessor(4096, 1, 1);\n\n            processorNode.onaudioprocess = function (audioProcessingEvent) {\n              if (!_this.recording) return;\n              var stream = audioProcessingEvent.inputBuffer.getChannelData(0);\n\n              _this.streamBuffer.push(new Float32Array(stream)); // set to a copy of the stream\n\n\n              _this.streamBufferLength += stream.length;\n\n              _this.analyse();\n            };\n\n            analyserNode = this.audioContext.createAnalyser();\n            analyserNode.minDecibels = FFT_MIN_DECIBELS;\n            analyserNode.maxDecibels = FFT_MAX_DECIBELS;\n            analyserNode.smoothingTimeConstant = FFT_SMOOTHING_TIME_CONSTANT;\n            sourceNode.connect(analyserNode);\n            analyserNode.connect(processorNode);\n            processorNode.connect(sourceNode.context.destination);\n            this.analyserNode = analyserNode;\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n  /**\n   * Start recording audio and listen for silence.\n   *\n   * @param onSilence {SilenceHandler} - called whenever silence is detected\n   * @param visualizer {Visualizer} - called with audio data on each audio process to be used for visualization.\n   */\n\n\n  AudioRecorder.prototype.startRecording = function (onSilence, visualizer) {\n    return __awaiter(this, void 0, void 0, function () {\n      var context, err_2;\n      return __generator(this, function (_a) {\n        switch (_a.label) {\n          case 0:\n            if (this.recording || !this.audioSupported) return [2\n            /*return*/\n            ];\n\n            this.onSilence = onSilence || function () {};\n\n            this.visualizer = visualizer || function () {};\n\n            context = this.audioContext;\n            _a.label = 1;\n\n          case 1:\n            _a.trys.push([1, 3,, 4]);\n\n            return [4\n            /*yield*/\n            , context.resume()];\n\n          case 2:\n            _a.sent();\n\n            return [3\n            /*break*/\n            , 4];\n\n          case 3:\n            err_2 = _a.sent();\n            logger.error(err_2);\n            return [3\n            /*break*/\n            , 4];\n\n          case 4:\n            this.start = Date.now();\n            this.recording = true;\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n  /**\n   * Pause recording\n   */\n\n\n  AudioRecorder.prototype.stopRecording = function () {\n    if (!this.audioSupported) return;\n    this.recording = false;\n  };\n  /**\n   * Pause recording and clear audio buffer\n   */\n\n\n  AudioRecorder.prototype.clear = function () {\n    this.stopRecording();\n    this.streamBufferLength = 0;\n    this.streamBuffer = [];\n  };\n  /**\n   * Plays given audioStream with audioContext\n   *\n   * @param buffer {Uint8Array} - audioStream to be played\n   */\n\n\n  AudioRecorder.prototype.play = function (buffer) {\n    var _this = this;\n\n    if (!buffer || !this.audioSupported) return;\n    var myBlob = new Blob([buffer]);\n    return new Promise(function (res, rej) {\n      var fileReader = new FileReader();\n\n      fileReader.onload = function () {\n        if (_this.playbackSource) _this.playbackSource.disconnect(); // disconnect previous playback source\n\n        _this.playbackSource = _this.audioContext.createBufferSource();\n\n        var successCallback = function (buf) {\n          _this.playbackSource.buffer = buf;\n\n          _this.playbackSource.connect(_this.audioContext.destination);\n\n          _this.playbackSource.onended = function () {\n            return res();\n          };\n\n          _this.playbackSource.start(0);\n        };\n\n        var errorCallback = function (err) {\n          return rej(err);\n        };\n\n        _this.audioContext.decodeAudioData(fileReader.result, successCallback, errorCallback);\n      };\n\n      fileReader.onerror = function () {\n        return rej();\n      };\n\n      fileReader.readAsArrayBuffer(myBlob);\n    });\n  };\n  /**\n   * Stops playing audio if there's a playback source connected.\n   */\n\n\n  AudioRecorder.prototype.stop = function () {\n    if (this.playbackSource) {\n      this.playbackSource.stop();\n    }\n  };\n  /**\n   * Called after each audioProcess. Check for silence and give fft time domain data to visualizer.\n   */\n\n\n  AudioRecorder.prototype.analyse = function () {\n    if (!this.audioSupported) return;\n    var analyser = this.analyserNode;\n    analyser.fftSize = FFT_SIZE;\n    var bufferLength = analyser.fftSize;\n    var dataArray = new Uint8Array(bufferLength);\n    var amplitude = this.options.amplitude;\n    var time = this.options.time;\n    analyser.getByteTimeDomainData(dataArray);\n    this.visualizer(dataArray, bufferLength);\n\n    for (var i = 0; i < bufferLength; i++) {\n      // Normalize between -1 and 1.\n      var curr_value_time = dataArray[i] / 128 - 1.0;\n\n      if (curr_value_time > amplitude || curr_value_time < -1 * amplitude) {\n        this.start = Date.now();\n      }\n    }\n\n    var newtime = Date.now();\n    var elapsedTime = newtime - this.start;\n\n    if (elapsedTime > time) {\n      this.onSilence();\n    }\n  };\n  /**\n   * Encodes recorded buffer to a wav file and exports it to a blob.\n   *\n   * @param exportSampleRate {number} - desired sample rate of the exported buffer\n   */\n\n\n  AudioRecorder.prototype.exportWAV = function (exportSampleRate) {\n    if (exportSampleRate === void 0) {\n      exportSampleRate = DEFAULT_EXPORT_SAMPLE_RATE;\n    }\n\n    return __awaiter(this, void 0, void 0, function () {\n      var recordSampleRate, blob;\n      return __generator(this, function (_a) {\n        if (!this.audioSupported) return [2\n        /*return*/\n        ];\n        recordSampleRate = this.audioContext.sampleRate;\n        blob = exportBuffer(this.streamBuffer, this.streamBufferLength, recordSampleRate, exportSampleRate);\n        this.clear();\n        return [2\n        /*return*/\n        , blob];\n      });\n    });\n  };\n\n  return AudioRecorder;\n}();\n\nvar visualize = function (dataArray, bufferLength, canvas) {\n  if (!canvas) return;\n  if (!browserOrNode().isBrowser) throw new Error('Visualization is not supported on non-browsers.');\n\n  var _a = canvas.getBoundingClientRect(),\n      width = _a.width,\n      height = _a.height; // need to update the default canvas width and height\n\n\n  canvas.width = width;\n  canvas.height = height;\n  var canvasCtx = canvas.getContext('2d');\n  canvasCtx.fillStyle = 'white';\n  canvasCtx.clearRect(0, 0, width, height);\n\n  var draw = function () {\n    canvasCtx.fillRect(0, 0, width, height);\n    canvasCtx.lineWidth = 1;\n    var color = getComputedStyle(document.documentElement).getPropertyValue('--amplify-primary-color');\n    canvasCtx.strokeStyle = !color || color === '' ? '#ff9900' : color; // TODO: try separate css variable\n\n    canvasCtx.beginPath();\n    var sliceWidth = width * 1.0 / bufferLength;\n    var x = 0;\n\n    for (var i = 0; i < bufferLength || i % 3 === 0; i++) {\n      var value = dataArray[i] / 128.0;\n      var y = value * height / 2;\n\n      if (i === 0) {\n        canvasCtx.moveTo(x, y);\n      } else {\n        canvasCtx.lineTo(x, y);\n      }\n\n      x += sliceWidth;\n    }\n\n    canvasCtx.lineTo(canvas.width, canvas.height / 2);\n    canvasCtx.stroke();\n  }; // Register our draw function with requestAnimationFrame.\n\n\n  requestAnimationFrame(draw);\n};\n\nvar amplifyChatbotCss = \".bot .dot{background-color:var(--bot-dot-color)}.user .dot{background-color:var(--user-dot-color)}.dot-flashing{width:2.625rem}.dot-flashing .dot{display:inline-block;width:0.625rem;height:0.625rem;border-radius:10rem;opacity:0.65}.dot-flashing .left{-webkit-animation:dot-flashing 1s infinite alternate;animation:dot-flashing 1s infinite alternate;-webkit-animation-delay:0s;animation-delay:0s}.dot-flashing .middle{margin-left:0.375rem;margin-right:0.375rem;-webkit-animation:dot-flashing 1s infinite linear alternate;animation:dot-flashing 1s infinite linear alternate;-webkit-animation-delay:0.5s;animation-delay:0.5s}.dot-flashing .right{-webkit-animation:dot-flashing 1s infinite alternate;animation:dot-flashing 1s infinite alternate;-webkit-animation-delay:1s;animation-delay:1s}@-webkit-keyframes dot-flashing{0%{opacity:0.65}50%,100%{opacity:0.1}}@keyframes dot-flashing{0%{opacity:0.65}50%,100%{opacity:0.1}}:host{--width:28.75rem;--height:37.5rem;--header-color:var(--amplify-secondary-color);--header-size:var(--amplify-text-lg);--bot-background-color:rgb(230, 230, 230);--bot-text-color:black;--bot-dot-color:var(--bot-text-color);--user-background-color:var(--amplify-blue);--user-text-color:var(--amplify-white);--user-dot-color:var(--user-text-color)}.amplify-chatbot{display:-ms-inline-flexbox;display:inline-flex;-ms-flex-direction:column;flex-direction:column;background-color:var(--background-color);border-radius:0.375rem;-webkit-box-shadow:0.0625rem 0rem 0.25rem 0 rgba(0, 0, 0, 0.15);box-shadow:0.0625rem 0rem 0.25rem 0 rgba(0, 0, 0, 0.15);-webkit-box-sizing:border-box;box-sizing:border-box;font-family:var(--amplify-font-family);margin-bottom:1rem;width:100%;height:var(--height);max-width:var(--width)}@media (min-width: 672px){.amplify-chatbot{width:var(--width)}}.header{padding:1.25rem 0.375rem 1.25rem 0.375rem;color:var(--header-color);font-size:var(--header-size);font-weight:bold;text-align:center;word-wrap:break-word}.body{border-top:0.0625rem solid rgba(0, 0, 0, 0.05);padding:1.5rem 1rem 0 1rem;display:-ms-flexbox;display:flex;-ms-flex-positive:1;flex-grow:1;-ms-flex-direction:column;flex-direction:column;overflow:auto}.bubble{max-width:100%;padding:0.8em 1.4em;text-align:left;word-wrap:break-word;margin-bottom:0.625rem}.bot{margin-right:auto;background-color:var(--bot-background-color);color:var(--bot-text-color);border-radius:1.5rem 1.5rem 1.5rem 0}.user{margin-left:auto;background-color:var(--user-background-color);color:var(--user-text-color);border-radius:1.5rem 1.5rem 0 1.5rem}.footer{display:-ms-flexbox;display:flex;-ms-flex-align:center;align-items:center;border-top:0.062rem solid rgba(0, 0, 0, 0.05);padding-right:0.625rem;min-height:3.125rem}.footer amplify-input{--border:none;--margin:0;-ms-flex-positive:1;flex-grow:1}canvas{margin-left:0.625rem;margin-right:0.625rem;-ms-flex-positive:1;flex-grow:1;height:3.125rem}.icon-button{--icon-height:1.25rem;--icon-fill:var(--amplify-primary-color);--padding:0.625rem;--width:auto}\"; // enum for possible bot states\n\nvar ChatState;\n\n(function (ChatState) {\n  ChatState[ChatState[\"Initial\"] = 0] = \"Initial\";\n  ChatState[ChatState[\"Listening\"] = 1] = \"Listening\";\n  ChatState[ChatState[\"SendingText\"] = 2] = \"SendingText\";\n  ChatState[ChatState[\"SendingVoice\"] = 3] = \"SendingVoice\";\n  ChatState[ChatState[\"Error\"] = 4] = \"Error\";\n})(ChatState || (ChatState = {})); // Message types\n\n\nvar MessageFrom;\n\n(function (MessageFrom) {\n  MessageFrom[\"Bot\"] = \"bot\";\n  MessageFrom[\"User\"] = \"user\";\n})(MessageFrom || (MessageFrom = {})); // Error types\n\n\nvar ChatErrorType;\n\n(function (ChatErrorType) {\n  ChatErrorType[ChatErrorType[\"Recoverable\"] = 0] = \"Recoverable\";\n  ChatErrorType[ChatErrorType[\"Unrecoverable\"] = 1] = \"Unrecoverable\";\n})(ChatErrorType || (ChatErrorType = {}));\n\nvar AmplifyChatbot =\n/** @class */\nfunction () {\n  function class_1(hostRef) {\n    var _this = this;\n\n    registerInstance(this, hostRef);\n    /** Clear messages when conversation finishes */\n\n    this.clearOnComplete = false;\n    /** Continue listening to users after they send the message */\n\n    this.conversationModeOn = false;\n    /** Text placed in the top header */\n\n    this.botTitle = Translations.CHATBOT_TITLE;\n    /** Whether voice chat is enabled */\n\n    this.voiceEnabled = false;\n    /** Whether text chat is enabled */\n\n    this.textEnabled = true;\n    /** Amount of silence (in ms) to wait for */\n\n    this.silenceTime = 1500;\n    /** Noise threshold between -1 and 1. Anything below is considered a silence. */\n\n    this.silenceThreshold = 0.2;\n    /** Messages in current session */\n\n    this.messages = [];\n    /** Text input box value  */\n\n    this.text = '';\n    /** Current app state */\n\n    this.chatState = ChatState.Initial;\n    /**\n     * Rendering methods\n     */\n\n    this.messageJSX = function (messages) {\n      var messageList = messages.map(function (message) {\n        return h(\"div\", {\n          class: \"bubble \" + message.from\n        }, message.content);\n      });\n\n      if (_this.chatState === ChatState.SendingText || _this.chatState === ChatState.SendingVoice) {\n        // if waiting for voice message, show animation on user side because app is waiting for transcript. Else put it on bot side.\n        var client = _this.chatState === ChatState.SendingText ? MessageFrom.Bot : MessageFrom.User;\n        messageList.push(h(\"div\", {\n          class: \"bubble \" + client\n        }, h(\"div\", {\n          class: \"dot-flashing \" + client\n        }, h(\"span\", {\n          class: \"dot left\"\n        }), h(\"span\", {\n          class: \"dot middle\"\n        }), h(\"span\", {\n          class: \"dot right\"\n        }))));\n      }\n\n      return messageList;\n    };\n\n    this.chatCompleted = createEvent(this, \"chatCompleted\", 7);\n  } // Occurs when user presses enter in input box\n\n\n  class_1.prototype.submitHandler = function (_event) {\n    this.sendTextMessage();\n  };\n  /**\n   * Lifecycle functions\n   */\n\n\n  class_1.prototype.componentWillLoad = function () {\n    if (!Interactions || typeof Interactions.onComplete !== 'function') {\n      throw new Error(NO_INTERACTIONS_MODULE_FOUND);\n    }\n\n    this.validateProps();\n  };\n\n  class_1.prototype.componentDidRender = function () {\n    // scroll to the bottom if necessary\n    var body = this.element.shadowRoot.querySelector('.body');\n    body.scrollTop = body.scrollHeight;\n  };\n\n  class_1.prototype.validateProps = function () {\n    var _this = this;\n\n    if (!this.voiceEnabled && !this.textEnabled) {\n      this.setError(Translations.CHAT_DISABLED_ERROR, ChatErrorType.Unrecoverable);\n      return;\n    } else if (!this.botName) {\n      this.setError(Translations.NO_BOT_NAME_ERROR, ChatErrorType.Unrecoverable);\n      return;\n    }\n\n    if (this.welcomeMessage) this.appendToChat(this.welcomeMessage, MessageFrom.Bot); // Initialize AudioRecorder if voice is enabled\n\n    if (this.voiceEnabled) {\n      this.audioRecorder = new AudioRecorder({\n        time: this.silenceTime,\n        amplitude: this.silenceThreshold\n      });\n      this.audioRecorder.init().catch(function (err) {\n        _this.setError(err, ChatErrorType.Recoverable);\n      });\n    } // Callback function to be called after chat is completed\n\n\n    var onComplete = function (err, data) {\n      _this.chatCompleted.emit({\n        data: data,\n        err: err\n      });\n\n      if (_this.clearOnComplete) {\n        _this.reset();\n      } else {\n        _this.chatState = ChatState.Initial;\n      }\n    };\n\n    try {\n      Interactions.onComplete(this.botName, onComplete);\n    } catch (err) {\n      this.setError(err, ChatErrorType.Unrecoverable);\n    }\n  };\n  /**\n   * Handlers\n   */\n\n\n  class_1.prototype.handleMicButton = function () {\n    var _this = this;\n\n    if (this.chatState !== ChatState.Initial) return;\n    this.audioRecorder.stop();\n    this.chatState = ChatState.Listening;\n    this.audioRecorder.startRecording(function () {\n      return _this.handleSilence();\n    }, function (data, length) {\n      return _this.visualizer(data, length);\n    });\n  };\n\n  class_1.prototype.handleSilence = function () {\n    var _this = this;\n\n    this.chatState = ChatState.SendingVoice;\n    this.audioRecorder.stopRecording();\n    this.audioRecorder.exportWAV().then(function (blob) {\n      _this.sendVoiceMessage(blob);\n    });\n  };\n\n  class_1.prototype.handleTextChange = function (event) {\n    var target = event.target;\n    this.text = target.value;\n  };\n\n  class_1.prototype.handleCancelButton = function () {\n    this.audioRecorder.clear();\n    this.chatState = ChatState.Initial;\n  };\n\n  class_1.prototype.handleToastClose = function (errorType) {\n    this.error = undefined; // clear error\n    // if error is recoverable, reset the app state to initial\n\n    if (errorType === ChatErrorType.Recoverable) {\n      this.chatState = ChatState.Initial;\n    }\n  };\n  /**\n   * Visualization\n   */\n\n\n  class_1.prototype.visualizer = function (dataArray, bufferLength) {\n    var canvas = this.element.shadowRoot.querySelector('canvas');\n    visualize(dataArray, bufferLength, canvas);\n  };\n  /**\n   * Interactions helpers\n   */\n\n\n  class_1.prototype.sendTextMessage = function () {\n    return __awaiter(this, void 0, void 0, function () {\n      var text, response, err_3;\n      return __generator(this, function (_a) {\n        switch (_a.label) {\n          case 0:\n            if (this.text.length === 0 || this.chatState !== ChatState.Initial) return [2\n            /*return*/\n            ];\n            text = this.text;\n            this.text = '';\n            this.appendToChat(text, MessageFrom.User);\n            this.chatState = ChatState.SendingText;\n            _a.label = 1;\n\n          case 1:\n            _a.trys.push([1, 3,, 4]);\n\n            return [4\n            /*yield*/\n            , Interactions.send(this.botName, text)];\n\n          case 2:\n            response = _a.sent();\n            return [3\n            /*break*/\n            , 4];\n\n          case 3:\n            err_3 = _a.sent();\n            this.setError(err_3, ChatErrorType.Recoverable);\n            return [2\n            /*return*/\n            ];\n\n          case 4:\n            if (response.message) {\n              this.appendToChat(response.message, MessageFrom.Bot);\n            }\n\n            this.chatState = ChatState.Initial;\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n\n  class_1.prototype.sendVoiceMessage = function (audioInput) {\n    return __awaiter(this, void 0, void 0, function () {\n      var interactionsMessage, response, err_4, dialogState;\n\n      var _this = this;\n\n      return __generator(this, function (_a) {\n        switch (_a.label) {\n          case 0:\n            interactionsMessage = {\n              content: audioInput,\n              options: {\n                messageType: 'voice'\n              }\n            };\n            _a.label = 1;\n\n          case 1:\n            _a.trys.push([1, 3,, 4]);\n\n            return [4\n            /*yield*/\n            , Interactions.send(this.botName, interactionsMessage)];\n\n          case 2:\n            response = _a.sent();\n            return [3\n            /*break*/\n            , 4];\n\n          case 3:\n            err_4 = _a.sent();\n            this.setError(err_4, ChatErrorType.Recoverable);\n            return [2\n            /*return*/\n            ];\n\n          case 4:\n            this.chatState = ChatState.Initial;\n            dialogState = response.dialogState;\n            if (response.inputTranscript) this.appendToChat(response.inputTranscript, MessageFrom.User);\n            this.appendToChat(response.message, MessageFrom.Bot);\n            return [4\n            /*yield*/\n            , this.audioRecorder.play(response.audioStream).then(function () {\n              // if conversationMode is on, chat is incomplete, and mic button isn't pressed yet, resume listening.\n              if (_this.conversationModeOn && dialogState !== 'Fulfilled' && dialogState !== 'Failed' && _this.chatState === ChatState.Initial) {\n                _this.handleMicButton();\n              }\n            }).catch(function (err) {\n              return _this.setError(err, ChatErrorType.Recoverable);\n            })];\n\n          case 5:\n            _a.sent();\n\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n\n  class_1.prototype.appendToChat = function (content, from) {\n    this.messages = __spreadArrays(this.messages, [{\n      content: content,\n      from: from\n    }]);\n  };\n  /**\n   * State control methods\n   */\n\n\n  class_1.prototype.setError = function (error, errorType) {\n    var message = typeof error === 'string' ? error : error.message;\n    this.chatState = ChatState.Error;\n    this.error = {\n      message: message,\n      errorType: errorType\n    };\n  };\n\n  class_1.prototype.reset = function () {\n    this.chatState = ChatState.Initial;\n    this.text = '';\n    this.error = undefined;\n    this.messages = [];\n    if (this.welcomeMessage) this.appendToChat(this.welcomeMessage, MessageFrom.Bot);\n    this.audioRecorder && this.audioRecorder.clear();\n  };\n\n  class_1.prototype.listeningFooterJSX = function () {\n    var _this = this;\n\n    var visualization = h(\"canvas\", {\n      height: \"50\"\n    });\n    var cancelButton = h(\"amplify-button\", {\n      \"data-test\": \"chatbot-cancel-button\",\n      handleButtonClick: function () {\n        return _this.handleCancelButton();\n      },\n      class: \"icon-button\",\n      variant: \"icon\",\n      icon: \"ban\"\n    });\n    return [visualization, cancelButton];\n  };\n\n  class_1.prototype.footerJSX = function () {\n    var _this = this;\n\n    if (this.chatState === ChatState.Listening) return this.listeningFooterJSX();\n    var inputPlaceholder = this.textEnabled ? Translations.TEXT_INPUT_PLACEHOLDER : Translations.VOICE_INPUT_PLACEHOLDER;\n    var textInput = h(\"amplify-input\", {\n      placeholder: I18n.get(inputPlaceholder),\n      description: \"text\",\n      handleInputChange: function (evt) {\n        return _this.handleTextChange(evt);\n      },\n      value: this.text,\n      disabled: this.chatState === ChatState.Error || !this.textEnabled\n    });\n    var micButton = this.voiceEnabled && h(\"amplify-button\", {\n      \"data-test\": \"chatbot-mic-button\",\n      handleButtonClick: function () {\n        return _this.handleMicButton();\n      },\n      class: \"icon-button\",\n      variant: \"icon\",\n      icon: \"microphone\",\n      disabled: this.chatState === ChatState.Error || this.chatState !== ChatState.Initial\n    });\n    var sendButton = this.textEnabled && h(\"amplify-button\", {\n      \"data-test\": \"chatbot-send-button\",\n      class: \"icon-button\",\n      variant: \"icon\",\n      icon: \"send\",\n      handleButtonClick: function () {\n        return _this.sendTextMessage();\n      },\n      disabled: this.chatState === ChatState.Error || this.chatState !== ChatState.Initial\n    });\n    return [textInput, micButton, sendButton];\n  };\n\n  class_1.prototype.errorToast = function () {\n    var _this = this;\n\n    if (!this.error) return;\n    var _a = this.error,\n        message = _a.message,\n        errorType = _a.errorType;\n    return h(\"amplify-toast\", {\n      message: I18n.get(message),\n      handleClose: function () {\n        return _this.handleToastClose(errorType);\n      }\n    });\n  };\n\n  class_1.prototype.render = function () {\n    return h(Host, null, h(\"div\", {\n      class: \"amplify-chatbot\"\n    }, h(\"slot\", {\n      name: \"header\"\n    }, h(\"div\", {\n      class: \"header\",\n      \"data-test\": \"chatbot-header\"\n    }, I18n.get(this.botTitle))), h(\"div\", {\n      class: \"body\",\n      \"data-test\": \"chatbot-body\"\n    }, this.messageJSX(this.messages)), h(\"div\", {\n      class: \"footer\",\n      \"data-test\": \"chatbot-footer\"\n    }, this.footerJSX()), this.errorToast()));\n  };\n\n  Object.defineProperty(class_1.prototype, \"element\", {\n    get: function () {\n      return getElement(this);\n    },\n    enumerable: false,\n    configurable: true\n  });\n  return class_1;\n}();\n\nAmplifyChatbot.style = amplifyChatbotCss;\nexport { AmplifyChatbot as amplify_chatbot };","map":null,"metadata":{},"sourceType":"module"}